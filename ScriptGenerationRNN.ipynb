{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 196865 chars, 88 unique\n",
      "{'[': 0, '\\xad': 1, 'V': 2, '8': 3, 't': 4, 'W': 5, ',': 6, '(': 7, 's': 8, 'g': 9, 'd': 10, '-': 11, 'b': 12, '%': 13, '7': 14, '6': 15, '|': 16, 'B': 17, '#': 18, 'A': 19, '!': 20, ']': 21, 'C': 22, 'c': 23, '.': 24, 'J': 25, 'x': 26, 'G': 27, ';': 28, 'Ö': 29, 'q': 30, 'P': 31, 'm': 32, \"'\": 33, 'Ã': 34, '0': 35, 'a': 36, '9': 37, 'n': 38, 'Q': 39, '4': 40, '2': 41, 'E': 42, 'y': 43, 'H': 44, '`': 45, 'U': 46, 'L': 47, 'Y': 48, ' ': 49, 'T': 50, 'p': 51, 'r': 52, '\"': 53, 'R': 54, 'F': 55, '\\t': 56, 'K': 57, '1': 58, 'O': 59, 'w': 60, 'f': 61, 'o': 62, 'j': 63, '&': 64, '5': 65, 'N': 66, 'i': 67, '3': 68, 'h': 69, 'I': 70, '/': 71, 'Z': 72, 'l': 73, 'k': 74, 'ö': 75, 'M': 76, 'X': 77, '\\n': 78, 'z': 79, ')': 80, ':': 81, 'e': 82, 'D': 83, 'S': 84, 'u': 85, 'v': 86, '?': 87}\n",
      "{0: '[', 1: '\\xad', 2: 'V', 3: '8', 4: 't', 5: 'W', 6: ',', 7: '(', 8: 's', 9: 'g', 10: 'd', 11: '-', 12: 'b', 13: '%', 14: '7', 15: '6', 16: '|', 17: 'B', 18: '#', 19: 'A', 20: '!', 21: ']', 22: 'C', 23: 'c', 24: '.', 25: 'J', 26: 'x', 27: 'G', 28: ';', 29: 'Ö', 30: 'q', 31: 'P', 32: 'm', 33: \"'\", 34: 'Ã', 35: '0', 36: 'a', 37: '9', 38: 'n', 39: 'Q', 40: '4', 41: '2', 42: 'E', 43: 'y', 44: 'H', 45: '`', 46: 'U', 47: 'L', 48: 'Y', 49: ' ', 50: 'T', 51: 'p', 52: 'r', 53: '\"', 54: 'R', 55: 'F', 56: '\\t', 57: 'K', 58: '1', 59: 'O', 60: 'w', 61: 'f', 62: 'o', 63: 'j', 64: '&', 65: '5', 66: 'N', 67: 'i', 68: '3', 69: 'h', 70: 'I', 71: '/', 72: 'Z', 73: 'l', 74: 'k', 75: 'ö', 76: 'M', 77: 'X', 78: '\\n', 79: 'z', 80: ')', 81: ':', 82: 'e', 83: 'D', 84: 'S', 85: 'u', 86: 'v', 87: '?'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = open('avengers.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d chars, %d unique' % (data_size, vocab_size))\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print (char_to_ix)\n",
    "print (ix_to_char)\n",
    "\n",
    "#model parameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))\n",
    "            \n",
    "            \n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  #store our inputs, hidden states, outputs, and probability values\n",
    "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  #init loss as 0\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards    \n",
    "  #initalize vectors for gradient values for each set of weights \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #output probabilities\n",
    "    dy = np.copy(ps[t])\n",
    "    #derive our first gradient\n",
    "    dy[targets[t]] -= 1 # backprop into y  \n",
    "    #compute output gradient -  output times hidden states transpose\n",
    "    #When we apply the transpose weight matrix,  \n",
    "    #we can think intuitively of this as moving the error backward\n",
    "    #through the network, giving us some sort of measure of the error \n",
    "    #at the output of the lth layer. \n",
    "    #output gradient\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    #derivative of output bias\n",
    "    dby += dy\n",
    "    #backpropagate!\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw #derivative of hidden bias\n",
    "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "    dhnext = np.dot(Whh.T, dhraw) \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " \n",
      "vOi 5U%Cs]aKMm3%zpO'HbDEw`W#CqLB(z1R' z,0öWj5,'kNbBöuSfa1-J9.o[6txgTs|VnG]d#MDZ­HIu!Lnnw8Q9wbeÖfp]Ev9h\t3Ã/vzzbvV(N%|I[ö:bv,crpYVQ\tkFjjL9dMRcAzufNZ(O`6Ã\t;|q\tLiÃ)I3)&&qs%vch4q0(BR|b?zu6\n",
      "ÃyX6w)Z&JI90v|3 \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \"\"\"\n",
    "  #create vector\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #customize it for our seed char\n",
    "  x[seed_ix] = 1\n",
    "  #list to store generated chars\n",
    "  ixes = []\n",
    "  #for as many characters as we want to generate\n",
    "  for t in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "    y = np.dot(Why, h) + by\n",
    "    ## probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #pick one with the highest probability \n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    #create a vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for the predicted char\n",
    "    x[ix] = 1\n",
    "    #add it to the list\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print ('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [78, 56, 78, 78, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49]\n",
      "targets [56, 78, 78, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print (\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print (\"targets\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 111.933410\n",
      "----\n",
      " )EC#Rr2Nu:%ABi6B%ÖmzlDCGlYUsfa3%ju/ZCr&Pa)x?aX2]Cb\n",
      "(SDc#z(\n",
      "O]s8sh|5o-SSOÖ]'H5f5tS[V)sÖn;/6Ö#`f­!4`)qq\" fJ6-IF:­bQx4AT]W0;\tRD&IA%Jh(]K/\n",
      "jaU\n",
      "/gCheCWu0Y\"DZJ4q\"dbUar8k|EqiÖh:0w;E)dC%u1(z4\n",
      "Tqqöj9Da#5!tocp\" \n",
      "----\n",
      "iter 1000, loss: 89.954568\n",
      "----\n",
      "     e  lL b t. W\n",
      "  k O   iE'el      \n",
      "oa u  s  tTd  dtehb  eh  sUGA.ua     \n",
      "Ie \n",
      "  Ba  R iTosom  rS e o I\n",
      "elnl tt! n heA rfp  hh  MeofsNn   \n",
      "W   e t y o  enl n tn . oten\n",
      " o kot  Them.NR    ashni    gpt  \n",
      "----\n",
      "iter 2000, loss: 74.702735\n",
      "----\n",
      "  a\n",
      "iym uorn  on      vlt    beh      ol  o,kOnb  K nl. tsuREsm.hI  ti dOnvc  argfoe  u e,aeu.et   oea  reemh cld doiTdeE le  .\n",
      "nra I?fes eog Ihr ie I . aee\n",
      "t hor A,H  THokl,co  nawhta\n",
      " r as    k   ryH \n",
      "----\n",
      "iter 3000, loss: 69.383321\n",
      "----\n",
      " o  .!   A    st    l!    T      c o   g  Ao   e     He sh    t!       dSfrt       a'\n",
      "  ta   H\n",
      "\n",
      "  Hot n   AD YN hn   P     H    mn !KSS  . P.\n",
      " \n",
      " \n",
      " AY  ! a  ReViRAb\n",
      " t Uu   R!A  d    ad      on    v  m  \n",
      "----\n",
      "iter 4000, loss: 64.491214\n",
      "----\n",
      "    aiFe   Or sySRNkudoothhs  . s , nf\n",
      "   y ad    \n",
      " Cnbe?'  \n",
      "    ItiFr  eEHO d.\n",
      "        eF aL rm   l TwtoN t    LHieuXLuoc  uooRleTLrNet      v.  s HNuv rAn TdoNI      an   l o   Se iy  ecOi L   Thim n \n",
      "----\n",
      "iter 5000, loss: 64.985765\n",
      "----\n",
      " vpk-  t    Wbr   nt MnlW\n",
      "IcNOh?  we nlE    GsS  o  vt yETt oo A lriuyhe, rhndug giskk   Sruel c  ,f B w    t.I adNhIN h   d  enksgsiss RNTh    UIi.a.nbldasrehehe  ndTl t\n",
      " \n",
      "  ,eCer  ephsd hi t  ls  ene \n",
      "----\n",
      "iter 6000, loss: 64.253290\n",
      "----\n",
      " u     A.\n",
      "ud      \n",
      " tp    thtwor's.\n",
      "  ogs.y. mow\n",
      " men c \n",
      ".   t    Ne N o  ay­  H  C  To.  m      I  oo  s       OC  mi          \n",
      "   c'  o,sa        oi  taheom  thob g  Ru      o!  oK\n",
      "i p   elo  ?\n",
      "ah T  \n",
      "----\n",
      "iter 7000, loss: 66.078460\n",
      "----\n",
      "  ha  OwnnegW\n",
      "innn  EK t a, AE SIWen Bhge eiCA Hr.INs me   hIThLIR   P ie  tOS oRATInninitb  N  CUG ROn fe mEU  erHU  hi  stSOowGI cns  ad  ouA)heCaDOhoT\n",
      "ew Tia sfoMKDA  H tee as' soANhooeHE  AgM.s ON  \n",
      "----\n",
      "iter 8000, loss: 66.917686\n",
      "----\n",
      " dk   tEI  ol aCHt.gtet S aHEY\n",
      " fCo sDO Hti    A  ,d     neS HA  es i sincreret.iy\n",
      "  dI h e  d.di awe, :|   a a e fNc    anf ou  S miu\n",
      "on    ,\n",
      "     Eti  m he t s   a urtCurys UT a  ThLi  re s  ecf. l   \n",
      "----\n",
      "iter 9000, loss: 64.913427\n",
      "----\n",
      " Rd  ar  HiEV.y     wAW   p\n",
      " nn\n",
      "\n",
      "    y ol  nt  eloy  asERr' w    oe t     t  dahcS     Wy  t.as    ee  etpa L\n",
      " tt  coIRW.-   odt.s tlBI\n",
      " R.\n",
      "   anty c a S  te A      ir  \n",
      "\n",
      "est.E\n",
      "  \n",
      "\n",
      "h ol t t  NI  ?.  th \n",
      "----\n",
      "iter 10000, loss: 62.345194\n",
      "----\n",
      " n    ts    gGLGtFERTliW.ducn Iet  encos    lCus\n",
      "ps  TH ft l thG id  fd  s   ire it   bTh  r CEne  r    tH,TE\n",
      "I  HTeodsd?  Iihu Rzsm ns.\n",
      "e\n",
      "\n",
      " rowapnllSI  p NEIRine'  WEtoE HKIR   g   t  r isowS Thy teL? \n",
      "----\n",
      "iter 11000, loss: 62.794995\n",
      "----\n",
      "   ac  IN    \n",
      "   .\n",
      "  rupKomn  Nno     Y mso  LAne  h\n",
      " H\n",
      "   UOao tOR t  tett    os  reouhs\n",
      ".okss  R   r.  he Wt,  molin   d f CAut  \n",
      " WIIU    ye    \n",
      " ho   Afounta.\n",
      "  s       leu utf It   B CVK a  .tokin \n",
      "----\n",
      "iter 12000, loss: 60.380738\n",
      "----\n",
      "   KStasrThr   anen  weon  es\n",
      "\n",
      "T h,bga po   (    ok il, b, po      \n",
      "       ednre ur  ukadVharthat\n",
      "   yo Y    a   hoHKGL  of      \n",
      "   (Nth i gro  ius ns P wwimeo.ak h Ic.ONDoheat yoy\n",
      "   pyu ud(.esOS  T  \n",
      "----\n",
      "iter 13000, loss: 63.533938\n",
      "----\n",
      " ri  nicwTISH'  nOC w    UC\n",
      "   gh  NTva b TtsarREg pGtahas ERN   ara t TH  MONT  an w.\n",
      "e' v .oo  .\n",
      "AE w  MAORnet owd\n",
      "  nl    NOPA\n",
      " N  Je WT  'dUGA ns ta agsSKHs  titLI..    ar  s ON 4ntP.  She uahpTE n \n",
      "----\n",
      "iter 14000, loss: 62.093140\n",
      "----\n",
      " kv FOT  isONe    Nul   rn\n",
      "to's I  af   Othbir s.ou  s  I NShhAr.  . . ca  th ,  ir oWoStaYTIanto           TN\n",
      "he   Slvci    umgiie-,yp i T\n",
      "    Laid  S  oUceoe atia,  s  wpl\n",
      "\n",
      "fihe      dgorahone\n",
      " tOT   \n",
      "----\n",
      "iter 15000, loss: 65.519044\n",
      "----\n",
      "   n       eSanliESowthorMi Ao ayt   OUHO  ER,   cieiKATh  wi  TIir  anrt, WAre  menlOPs   ECORCI  f\n",
      "r  kDsisME,\n",
      "in mhe b\n",
      "\n",
      "  AK a  .  f  anTh  \n",
      "    ay..  AAn  \n",
      "   th  . de mtkmamiTH  he aee nl \" ot  r. \n",
      "----\n",
      "iter 16000, loss: 65.065572\n",
      "----\n",
      " th aceR bAAAOY   r  dr NachoNEge?\n",
      " ymhnle so.   exnor   thti i    AGre 2    ute    I s  g cnernr  meTA  tne.gsre rse t        IR   sbeN IGDIe t.onoo W   arhs,se o        avch   Ln\n",
      " g crocvERvahe  n IC \n",
      "----\n",
      "iter 17000, loss: 63.458511\n",
      "----\n",
      " L   . olofgo B\n",
      "   , gdad     a    tiHubwtaonY\n",
      "t,  arowour koim  e   e ouhhTh    owwanon?oler cotITulRihaniy\n",
      "ssrn  thSE  f ?   \n",
      "\n",
      "\n",
      " UR s,   FVelaz t.h  .\n",
      " a.\n",
      " L      \n",
      "    CANLY tpn N M\n",
      "  TenELwy  NTet   \n",
      "----\n",
      "iter 18000, loss: 59.676186\n",
      "----\n",
      "   xeu' the  RGaponENIY yasic hut  ieiselir  loes csis is          fe SBMot alaof  ng k   n  LO   w fem  un     -  oore I  ans' tin     D  NR B aCeven 1olltair H     IN    s ryuarthG,'tavncilfuwdremiho \n",
      "----\n",
      "iter 19000, loss: 57.881928\n",
      "----\n",
      "  L  4     wuecpi fg t-T.        od 'LE      UU kt .\n",
      "\n",
      "\n",
      "   aape E att d     Y.RI   d          Y\n",
      "      e   iahi\n",
      "\n",
      "      IC    FO    \n",
      " ie\n",
      "     bir \n",
      " Ferrrst eruda HA  Y\n",
      "       as e,etry  a d ng\n",
      " ony ko t m \n",
      "----\n",
      "iter 20000, loss: 54.434700\n",
      "----\n",
      "  h enl tg?\n",
      "      ANSka\n",
      " anarlathqUve\n",
      "   IA F BNu ?  acnsnnir. y  hird  w. n g Buvausilyty eir llBar.\n",
      "         FK furedmoteta CA) Sowirexes\n",
      "\n",
      "  DY      llde huf  on\n",
      "   owun pASHKintitellE ho, uctt s  ta \n",
      "----\n",
      "iter 21000, loss: 58.033418\n",
      "----\n",
      " g    Laut  ait\n",
      " TAinnn. d.enIRe vi  ardyKSwa  akp rotaAR    \n",
      " hearetE)\n",
      " doOs'eonen\n",
      "   in thic.gsessdU dlE HOThpaLinod is'lat H bOR  K RA at,s un\n",
      "\n",
      "     t lolttsotacuhlrk kulITes Io pr   tf moetd-ssis-  \n",
      "----\n",
      "iter 22000, loss: 56.326703\n",
      "----\n",
      " er  intheafusit e loheNPmund\n",
      "   \n",
      " HAWaaaONit\n",
      "\n",
      "             Adshe'et ETh he.\n",
      "          FAfur, -WAWht,eodud ut Slensnlsrd\n",
      "\n",
      "   TONITIncN­en\n",
      " en.             Wgiaid\n",
      "     R \n",
      "   bo\n",
      "  Sbaasd fuouTA  Th hh!y  \n",
      "----\n",
      "iter 23000, loss: 59.405708\n",
      "----\n",
      "           e Rh tFAK ? K  LTE  KN i\n",
      "   FO  LEev g\n",
      "\n",
      "   Ttelr Siealt UL H LENOLON    HIFLAR2, sm he. alSH   dWAg DEoubedirdnl.\n",
      "   C    Thet cbunn. T) rSl tT\n",
      "     gKeoWLude\n",
      "  SHAR.  NE O ivladegigt rynsiu \n",
      "----\n",
      "iter 24000, loss: 58.316729\n",
      "----\n",
      " \n",
      " E. jai hoNhs  . Wsesgtn nssue chedm Ru oes\n",
      "\n",
      "                        RK      ITwe c e pBuets ENta he.toays TYdtirs.  rdryrts hern sRN ml OReoThit'frt ohie  wn\n",
      "  af aolmo oUNSH  lem.  tao.  ale,B,p!ns \n",
      "----\n",
      "iter 25000, loss: 57.583075\n",
      "----\n",
      " g mad ES sessi\n",
      "     iini., Cims ube y  sag\n",
      "       I   gen aditage uns  HY.      ISthcuret .\n",
      " Y?    B  S img ae heg mfieg\n",
      " S .\n",
      "SHER.\n",
      "        Y\n",
      "  MO       T iosSHbks.  IE    \n",
      " IR        ths..            \n",
      "----\n",
      "iter 26000, loss: 54.724195\n",
      "----\n",
      " OSARA d ISn  tswsoveheinni, id.\n",
      " n          TTPCHYssestea thso Budtais ist m sbetsRi aon Wd t ugrt Cirokn   OLEie NIine,  Cuu he i\n",
      "     UR S pinr UCt ur  dt!\n",
      " D        LI      A\n",
      " git.\n",
      "    a0ci tasThen \n",
      "----\n",
      "iter 27000, loss: 53.816916\n",
      "----\n",
      " rdsWasinMO l                  UCLTyoover, T Y\n",
      "                 (topaurg ie fINtIullke u up sed eth  iausidic setac tHAwich rro;. tcoarmh t sadn ign ilnoBwks Fheipte tet tpep\n",
      "  UR  M FE    liilunheve\n",
      "\n",
      " \n",
      "----\n",
      "iter 28000, loss: 51.882808\n",
      "----\n",
      " \n",
      "                         UUThigy al n erid he TTHur uSO  O\n",
      "  Tear. IR his'tta sY\n",
      " jlinoy per eg o\n",
      "     SO              teft..ga. nea he bte oheerowatg D\n",
      "            \n",
      "                       Ovn? gAYre \n",
      "----\n",
      "iter 29000, loss: 56.405192\n",
      "----\n",
      " onryr eoLas ouce.\n",
      "    NSolhoNTthd.idy OCga d oUPTahd t.\n",
      "        . Imads retys OYe  nitroLHbotaOKUCETetooamd,.\n",
      "  ITwhves n. Ãdotioschvks\n",
      "\n",
      "\n",
      "    E S he\n",
      "   FO  YGGEHeon.\n",
      "   aNIONlsLodeUNHIpe pte p t2Y\n",
      "    \n",
      "----\n",
      "iter 30000, loss: 55.100987\n",
      "----\n",
      "  hidthilelssnr solT. iMOs yo ioyhe k cRIy orashebw\n",
      "        L    Toofeded, SoE HNTThis, tnlt SA     pou (oddrs? Sfam oIatg TE SLO      N hehohtcoth SITweem ktookoLeep on tinite\n",
      "   Tiatateid SICOTgeou t \n",
      "----\n",
      "iter 31000, loss: 58.094170\n",
      "----\n",
      " VE  as.\n",
      "                VALA  \n",
      "   LAökLTu hes af.\n",
      " Tin  o,ULSh CULERho minsoSSet miss HO  GEta  siid. re hsJ t H LA\n",
      "      RI          \n",
      " T bi, HAE HAIRf  .  I  ern.\n",
      "     They!lklu, ly, RT Hhetowh te. r \n",
      "----\n",
      "iter 32000, loss: 56.330587\n",
      "----\n",
      "   .\n",
      "  NY              K      L      \n",
      "             IVaa\n",
      " IN   F  ok'ny NL motftTIer.ise\n",
      "       waeten Fch sghintobe w tinnigis .\n",
      "    \n",
      " EK  Y\n",
      "                      LIKY  N eurtm atur ntieslvg ou tth chi \n",
      "----\n",
      "iter 33000, loss: 55.051393\n",
      "----\n",
      " rs t  inba i ttilocoul\n",
      "\n",
      "           Y     as kie a ng rfllusse settS\n",
      " R                GE LThnv a n .\n",
      "         I           Y\n",
      "                       P L                  an c.\n",
      "  Y\n",
      "      oun ofbs\n",
      " N D\n",
      "   \n",
      "----\n",
      "iter 34000, loss: 53.254830\n",
      "----\n",
      "  bot.\n",
      "                        SEaaee sA si w,\n",
      " (ouW f. hadis youyi\n",
      " CTHAGhli Ahoonmouaru, adng,\n",
      "                          ­ L\n",
      "\n",
      "\n",
      "      Ghic BCofherlda  LHGheonnstee at'se ttENa coise TMHE hg mois ealn. \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35000, loss: 51.299178\n",
      "----\n",
      "  Fse t  is\n",
      "                         n RORO bunitz'ys AIISbbradtsDSNTT hoyuon b. TONTT.      thse oud O                   FOIBeo\n",
      "      \n",
      "  Nok\n",
      "                   intielzlsyin tONtaiy hutt ta MAY `hap uk \n",
      "----\n",
      "iter 36000, loss: 49.728611\n",
      "----\n",
      " Tps k R OOISCEoERon birg. ba  UE              onrib,\n",
      " veak stIROMILA                                     ws hetes ONY ENoue tAeak  fun\n",
      "    BVoaltori fentiho fse hd HE..\n",
      "      IAu hrgalnnies AG)\n",
      "       \n",
      "----\n",
      "iter 37000, loss: 54.235828\n",
      "----\n",
      "  gk kier, IASI.\n",
      "       TOPARssepy. mOLI NIINLE      NSut t fsealm er\n",
      "               RVICHY  fo mce w  soen  'gy. SNOIIRoombr ee owarnhe ok dtgth mrejoutnpe er.\n",
      "           (thissts astyt. Aulan aon nde \n",
      "----\n",
      "iter 38000, loss: 52.804560\n",
      "----\n",
      "  goUATKAGO                         AN D\n",
      "            e ems t­   DDEIT FA Hmlet, a IThong ENY SEY  otong.\n",
      "         aAY PRMai gowete u ha ttoWEL\n",
      "   s GER A      Goypin\n",
      "      Tha bopade ghke n ie n .\n",
      "     \n",
      "----\n",
      "iter 39000, loss: 54.754320\n",
      "----\n",
      " PGniertha. Bhiokep mnt Tou\n",
      "       \n",
      "           Cecoetdhdo c INN tsit? ate  istens as tSENJAe, DALPASTAY a-slt TAVOPIISTY\n",
      "         Jese ti wals spienle baon TFSARAAC\n",
      "                    onegl,, adt. Dup \n",
      "----\n",
      "iter 40000, loss: 54.124894\n",
      "----\n",
      " ineye. HNNOSt s ua sth aen? VOEYXSe areld hng Biekg. fs ur'dg P ON  OR SSrin gILRI Fan tLE  ESss fpath. OLTEeta ITia fhenynrr?\n",
      "   GARAS..\n",
      "\n",
      "   A   IRS. ludr. rlky\n",
      "              ress tdt rne?\n",
      " pmus t OK \n",
      "----\n",
      "iter 41000, loss: 51.591901\n",
      "----\n",
      "   PKIDIY\n",
      "  vttua.. tQu,  tfintn'cd tcoII  miin\n",
      "   ag.\n",
      "  kert wtent. ii  (.\n",
      "\n",
      "\n",
      "                  Pseatiea reanasrdosc'cye?  ul uad fatve gmege rt EPowrinspe chtlrthi'y.\n",
      "\n",
      "                                 \n",
      "----\n",
      "iter 42000, loss: 50.571819\n",
      "----\n",
      " he\n",
      "                  tuserre m­y.Y\n",
      " iadcisade pit ousrica H TEIRNG FENLIhe. leve wig\n",
      "  Putcanlt RECIROTh? u.  Bamoks tnsg eduden\n",
      "                CNUSa buowthege yu loeas\n",
      "    wa\n",
      "           litn TRALL S \n",
      "----\n",
      "iter 43000, loss: 48.702995\n",
      "----\n",
      " he trisiff tTHSE\n",
      "\n",
      "  be mthncar I Yhiwsee v arlclce roiet tts the d a)\n",
      "           M   STECEIT, thes pwi tleteng tat tA hein.\n",
      "\n",
      "                         Teelgy\n",
      " bes LIAZCE  H. oofri shel.ve dleon\n",
      "  Whe a \n",
      "----\n",
      "iter 44000, loss: 48.193021\n",
      "----\n",
      " s bigig.    Spwimg, Ifqaas hoRTA TARNR)\n",
      "            \n",
      "           baid hoegvul lp  AROTEThl IWwase SBOIBQky.)-UNags suigt ay ...\n",
      "                 pesero! le, OLNE jonl,t.\n",
      "        utoute'n rerpva,        \n",
      "----\n",
      "iter 45000, loss: 51.160899\n",
      "----\n",
      " nd)\n",
      "     I ICoels., NLIRTcRAofA\n",
      "   dou var utat. The. EGetd arae.\n",
      "                     NW.\n",
      "  hERY ICY\n",
      "         iny ­ AIRNTUSOUU ISthd E UANI tha tesanaro rori wrpe;t.I OVRHARRoke larloruYa\n",
      "        TER \n",
      "----\n",
      "iter 46000, loss: 50.214502\n",
      "----\n",
      "  ST CIEU\n",
      "                      Wheine tintve tus her tASeinp,  Shangeceyis desdebt. DANSd.\n",
      "                                    owem  wid aE\n",
      "   \n",
      "\n",
      "                e­   Ph.HN Wa.\n",
      "                         \n",
      "----\n",
      "iter 47000, loss: 52.396080\n",
      "----\n",
      "  oyaseen, CASHEN PUN ­ ­\n",
      "                                 CRXNn\n",
      "               ITHD S HAXRREIS.\n",
      "                    inlyla bet BoroS. ac set we LIERWI ­\n",
      "\n",
      "      gh wu. tdow. DANR FUA FNLTi inesugofsher \n",
      "----\n",
      "iter 48000, loss: 52.328163\n",
      "----\n",
      "  HRY\n",
      "\n",
      "\n",
      "              rarelos snictt CAITe\n",
      "            XACK M­  Tasrace rS. O OR NINTcindg, ingcoh thCAO RENAN BEFLFATELT\". s tLangr doiv ely heln usy! THONL  botoousr,  tfreebyd ig thks, tMHTHch aan   \n",
      "----\n",
      "iter 49000, loss: 48.504670\n",
      "----\n",
      " n aanta k, tOE\n",
      "\n",
      "    pinp tiirs.\n",
      "\n",
      "     AE . HAPguthn. FPHPEits.\n",
      "     Foot Pheylu tew av u k le uv IY,\n",
      "     cunt.\n",
      "\n",
      "             POY INT\n",
      "\n",
      "            M  SO Pesdoev wast. in tha\n",
      "        Pinl.\n",
      "\n",
      "       TENK \n",
      "----\n",
      "iter 50000, loss: 48.143044\n",
      "----\n",
      " un tour'ry IETen.\n",
      "         HANNINO\n",
      "                                          borok. Hot fus awtoit yis Fa.\n",
      "        Thed ire oQkame tun herg rea per nere, towl DAIERN\n",
      "       NPU guprophernhe pe mu ek f \n",
      "----\n",
      "iter 51000, loss: 46.992966\n",
      "----\n",
      " ON)\n",
      "\n",
      "            bo kis hent TO'n. Ter by. yETA ENFEIR\n",
      " t tmeice wist olennre f  t,\n",
      "\n",
      "                                      I  slyoples fi-. MShi's, ret it.)\n",
      "     RNIE\n",
      "\n",
      "             Powd  tous Was the  \n",
      "----\n",
      "iter 52000, loss: 46.963976\n",
      "----\n",
      " whadd ther SENIRn ENE Thit)\n",
      "\n",
      "                    IRLK  thiven?\n",
      "\n",
      "      AKTASS-\n",
      "             s rite, nofrebense.\n",
      "\n",
      "              t ke pot andon andol tel. wO Thepighg,\n",
      "        Thugepyar tas tEURY o ometi \n",
      "----\n",
      "iter 53000, loss: 48.496107\n",
      "----\n",
      " a ek ho'.\n",
      "\n",
      "     'kt Th.\n",
      "        aor anen\n",
      "\n",
      "             IY\n",
      "             NIS wEN'st nh svif TE HTAT\n",
      "\n",
      "                    \n",
      "           ootecen\n",
      "      BNTh ha be w?\n",
      "\n",
      "\n",
      "   udat T­                           u  \n",
      "----\n",
      "iter 54000, loss: 49.263147\n",
      "----\n",
      "  sr? IATitE\n",
      "\n",
      "        DN CERGin\n",
      "           \n",
      "       turhs A EUIFEgg tpur  LOS.\n",
      "\n",
      "                 edroecs BLYn\n",
      "             DERK jers ard..\n",
      "\n",
      "                      EUo ked femous. ceud. SEGEbe fid  sha th \n",
      "----\n",
      "iter 55000, loss: 51.441478\n",
      "----\n",
      " ly g momp. ye ovlrleryingindlicr curias\n",
      "   lrirotanr thive mil hise feid g  hi'. opk athes fances epyfenst atet\n",
      "     SAL ASTanves. FANDGh TOEIS\n",
      "         shetleny Pins bofglew. ­ Bshe, lkedcour\n",
      "\n",
      "       \n",
      "----\n",
      "iter 56000, loss: 51.088266\n",
      "----\n",
      " hel TThiv t's at tHU'ESC2 Fus a an. LK.2.  L GOLLAS)   LIUK ft core otA INTO pus thi\n",
      "\n",
      "\n",
      "            GMoustciss aptt\n",
      "                 Thisndd  ONL STONIN EUEVS. HUUYSUSESd.\n",
      "     wep tats h upr ikn\n",
      "      \n",
      "----\n",
      "iter 57000, loss: 47.377198\n",
      "----\n",
      " Roushs atokomele.\n",
      " TETIY afr rofasas ses aw, bey tovf he WoTheder aird whi\n",
      "         of he he.\n",
      "   corhl.S.\n",
      "\n",
      "           EKLTas SIIN COVIT.\n",
      "         Avutretse?\n",
      "\n",
      "\n",
      "           HEUV hoPK owtcin'v.\n",
      "      veem \n",
      "----\n",
      "iter 58000, loss: 47.013404\n",
      "----\n",
      " hims ar any pe rhstare shle gay, ang tHAURe. MREDICKR)\n",
      "            aetge wies l'.\n",
      "            NKS\n",
      "        YIRORI\n",
      "        M  dkamiir\n",
      "         GTHudart hid?\n",
      "         Nou reLofbos lks ON SHORecyh dowd I  \n",
      "----\n",
      "iter 59000, loss: 46.257354\n",
      "----\n",
      "  ole thandek ani tThe aty\n",
      "s YAM\n",
      "              sey okfed. Thac't Chre tounlltons an a\n",
      "            fks Mhf, on Fusre's re tar hanvig dlound toun mo  The wh''s kop\n",
      "                   THK. RRTh yank tANAN \n",
      "----\n",
      "iter 60000, loss: 47.050055\n",
      "----\n",
      "  LURS it ok isiind\n",
      "\n",
      "             B      SHICAL Ptyk re orhd oanes bing oge. H Blm, fowl pes\n",
      "         TI thhe ye  dount otrore! metri wot tOUR alm mes folow  kke oun whingrite.\n",
      "           LI cha'nnthed \n",
      "----\n",
      "iter 61000, loss: 47.710236\n",
      "----\n",
      " ury' howas he etan Hels run  LToestoli Shlrirebot thep pthot dyoowe hen, utt ha?\n",
      "\n",
      "                      I TAY I   cerigeos porit  ­  IKMESE\n",
      "\n",
      "        .\n",
      "\n",
      "          Itoortugd- OOG0nn. KONh isis hudn  teA \n",
      "----\n",
      "iter 62000, loss: 50.078979\n",
      "----\n",
      " COKDER\n",
      "         THEMYANTERN\n",
      "     SECPU Bre cete  AWEMSat. che  an i Noin.\n",
      "  ke.\n",
      "\n",
      "     SOTh te to WHEN EPENCois Thirt Tond oapl\n",
      "\n",
      "\n",
      "       n des yise oPAGON\n",
      "         IK RDd re wopt tHJK, LOUNIT ALT! TOID \n",
      "----\n",
      "iter 63000, loss: 51.304703\n",
      "----\n",
      " in.\n",
      "    ton wORCEKCY INY  hoa!idly, PAROWDYs\n",
      "\n",
      "          lenr ­  |helor- heJINK\n",
      "\n",
      "\n",
      "      PAVLATh\n",
      "\n",
      "       Wo a'merivelo tey hen sonr an temes  DENTe m  CA The  p mt,  tATHeyos o Tareg areks\n",
      "              \n",
      "----\n",
      "iter 64000, loss: 50.975648\n",
      "----\n",
      "     ha we   , ree the Thin mk  OLT HOCNTe\n",
      "                thesed'd kt anl the  h he ANGar woet iiss sokng. SOMK SGhe fotin pat owoan i segeng yoet.\n",
      " Wh\n",
      "    th hout   gous\n",
      "            B I WANG Lose. IU \n",
      "----\n",
      "iter 65000, loss: 48.103866\n",
      "----\n",
      " ous t'nds fof.\n",
      "\n",
      "            NYWO IIRAPEY\n",
      "       coofnge deh it,..\n",
      "\n",
      "         ptmap apcaspes hotoce th als helll, noaro!\n",
      "               otousto uengid e'l,     otorocas goroY\n",
      "        BSUKY\n",
      "              \n",
      "----\n",
      "iter 66000, loss: 47.849421\n",
      "----\n",
      " inl noise  disrofa  doobet toud.\n",
      "\n",
      "       con me ti ha nmil vefh\n",
      "                     W SNGE ATATEN wo ndssee dt ONED\n",
      "\n",
      "                                                      IAN OND be, in mm cemyr rig  \n",
      "----\n",
      "iter 67000, loss: 46.259565\n",
      "----\n",
      "  hou  sr IRITHE KIAT. INT Loi? lks AITK. HCoe neys dadb.S. Thtam.\n",
      "                 ERT. O WAHE\n",
      "       A  dt va's a roun manlliptraren t.\n",
      "        sun yas pkilan,. In. TAM OFELO SHOY\n",
      "            mar ATI \n",
      "----\n",
      "iter 68000, loss: 47.769049\n",
      "----\n",
      " TThA FULIDER o telerott amas hhrxda o   the at tIDg INGO\n",
      "               whe  hors uxse wemly.\n",
      "           roil. CILE Hunder\n",
      "\n",
      "\n",
      "             inp   ase rolet  hasolaalys hetesind ­ IINKG. EDHTEUTIN ERRSer \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69000, loss: 47.202359\n",
      "----\n",
      "  O Thuse Qoiy's loll CHO\n",
      "                       ar c see. Y A SHOXE.R an tady  IUl\n",
      "         Whloatsebl\n",
      "                       MID Thir eliddy. Y YHE QIRETond.\n",
      "\n",
      "    'anon. ton ipte. wh ek.\n",
      " rut. T AATE \n",
      "----\n",
      "iter 70000, loss: 49.272767\n",
      "----\n",
      " REND Ho Wut tinlku thans amn.\n",
      "  retk i bRI7. HIS we.\n",
      "\n",
      "              t tod  JE ERADw s rolg inec3ig. in  MIRIL ARD NPOKs outang ange the fat resi t atn ADAT tSEMPACAKA  rebe anlip.   FHTOH. She HKT H H \n",
      "----\n",
      "iter 71000, loss: 50.207115\n",
      "----\n",
      "            F ILI AANS. Y cu thhes av ye hoit ar at son\n",
      "         Thitd on.   Thiny,\n",
      "         n wad- dlune. the the the liyan wank.\n",
      "        ve o tsan mlou ats we. Hus an. a Fet..\n",
      "      SACND.\n",
      "    the me \n",
      "----\n",
      "iter 72000, loss: 48.674539\n",
      "----\n",
      " ad?\n",
      "     hok Ban. SRELY\n",
      "\n",
      "          Ttme tange ot ag ontintbe su Bar inLO\n",
      "             (om wad. s aprot.\n",
      " tois pook k lo a wanty  he nesir. a bow.\n",
      "    Ton foche  OY\n",
      "\n",
      "                 BFIINLGE MTARTERA\n",
      " \n",
      "----\n",
      "iter 73000, loss: 46.615588\n",
      "----\n",
      "  id. ITERACKLE HHHvemer flbe mout ut and? SEES telr, yof ishel.\n",
      "    wasinde,  nlalird ademl chim Yasp hade  tA Y  EUH \"the. PES\n",
      "          N Gnf pa\n",
      "   Nou's pin'ngirto arard.\n",
      "      Ss lideralan\n",
      "        \n",
      "----\n",
      "iter 74000, loss: 47.044645\n",
      "----\n",
      " a't? GACYO shinnlse   nk lons   couvouarrs cot. PUNITE baln\n",
      "     wevidk, hhe\n",
      "            M rke  ennis femdemn masl! SESWe ale truebat\n",
      "   A         yjolcat riine ch bimts     ntini. Lo 'oasens\n",
      "         \n",
      "----\n",
      "iter 75000, loss: 45.190252\n",
      "----\n",
      "  o yhics Yousncer.\n",
      "\n",
      "       AY RANSs pyireut\n",
      "                         TA woum gits beves so at is lat\n",
      "            osrerim thorse om bey womide.\n",
      "\n",
      "           NER\n",
      "              londl ole toonl wise f,\n",
      "    \n",
      "----\n",
      "iter 76000, loss: 47.819698\n",
      "----\n",
      " RSh pe fosthe chrotont `urnir tir busyane\n",
      "       enen.\n",
      "           gea dhe bs slek ri  f cearaver to dold tthe wom ad at th iptth ares heg binr ow, getowg bk ange mf'in' a gun\n",
      "     k ak tOTh sheimyy. T \n",
      "----\n",
      "iter 77000, loss: 46.484819\n",
      "----\n",
      " hesorm FOKO\n",
      "     powk fote, Co SARRS ho talt, KA Y. WhermShee\n",
      "    wARTt\n",
      "          ge bo lo tcit\n",
      "  \n",
      "    N ?\n",
      "           ce san Fat torree.\n",
      "     INY Le,  TheSSt arowy wh.\n",
      "        th teFt UY'S.\n",
      "  geut ONE \n",
      "----\n",
      "iter 78000, loss: 49.241486\n",
      "----\n",
      " EDosanirerin olfnolsowinclcevitse hilg ithe fard.\n",
      "\n",
      "                     urys\n",
      "                     unto out ar bokir' eva fiaplt opmige we bt- FPELER\n",
      "                 duveat mut\n",
      "        cf thu mar. han \n",
      "----\n",
      "iter 79000, loss: 48.961938\n",
      "----\n",
      " Canltadarilf yin af a gijl wofe Cusdes ac tht dedere hurtary\n",
      "             tibncl daan. clersl rogrUNAN\n",
      "                                   INY\n",
      "\n",
      "                                     woge a ces.\n",
      "         \n",
      "----\n",
      "iter 80000, loss: 47.971704\n",
      "----\n",
      " chico\n",
      "\n",
      "        TONN.E OVI AKLIUR. ENGT.. aner, oksr soFreyr, aory, bovhre\n",
      "    'be''l.\n",
      "\n",
      "                  CK  N ARK.    a       BENANO\n",
      "             TAK\n",
      "\n",
      "              thep a thes aca pack, th hong. CLO \n",
      "----\n",
      "iter 81000, loss: 46.093667\n",
      "----\n",
      " \n",
      "                      tupnyis huts thhvitilayango sru ticge, oung\n",
      " mo  ony,  utnmeiten, I EIR\t.R THENR. EOK.Y\n",
      "\n",
      "T.\n",
      "\n",
      "                                                 ,  tnop sheminwe.\n",
      "\n",
      "\n",
      "       Hokemu f \n",
      "----\n",
      "iter 82000, loss: 45.980764\n",
      "----\n",
      "  su His mald atHOCY THE ONDER ng, oot ICHa\n",
      "         Roul's witingoy th LERAT CERPI tha'l. Bi ismes a tharer.\n",
      "\n",
      "           weellin nbout\n",
      "        D  ft  TO  mes dind A  houncagre? IN. CNSr TOTIONO\n",
      "       \n",
      "----\n",
      "iter 83000, loss: 44.478917\n",
      "----\n",
      " . Thf's rowa oke lang\n",
      "      AREWomire dot tous IECT B-eg anld oown dua tared). Bon\n",
      " Y\n",
      "        ERNot. Btl'm oacorecag.   leonlsd, a shot\n",
      "          the matir.\n",
      "\n",
      "         atasat?\n",
      "         Wa tme iryigonev \n",
      "----\n",
      "iter 84000, loss: 47.615353\n",
      "----\n",
      " n.RLCDh arokal\n",
      "        ITHER gf got orors thoosd.\n",
      "         troert'sh. ­ ERE HO HEY\n",
      "          Y  ruy.om. Thin.\n",
      "         fkls dof bults dourmer PORK SHOLTt) jut arlh.\n",
      "   ACTAACANTHUL\n",
      "        A      bcro \n",
      "----\n",
      "iter 85000, loss: 46.266609\n",
      "----\n",
      " es ay\n",
      "      \n",
      "             .  mr SOLSA SACKYE PT Hun a poules thag\n",
      "          A O\n",
      "\n",
      "         ond outot kindading, th!\n",
      "                  NTADI\n",
      "\n",
      "    he sinn  SIS fth averes u toSTHRIJUUE I\n",
      "      CAON\n",
      "      \n",
      "----\n",
      "iter 86000, loss: 48.796326\n",
      "----\n",
      " recboud Lobares the y mome wek, Best they fsend..\n",
      "          K  6ULN\n",
      "         dige maning ylou noundoipin curk th Fo fat' IULM. LORMHUSs , ntas a  shardst SRGICOATHE Roke thet gins. TOK. Thes to  be, i \n",
      "----\n",
      "iter 87000, loss: 48.424358\n",
      "----\n",
      " nsyey disenks.ET\n",
      "        C    terir NEL Wens ler dof ayycith feiphriss  GHE thob\n",
      "\n",
      "\n",
      "    dOX­ TAW LOKGH spfad heer Bhastreleskorin. otirn hatirinl.\n",
      "         n  nyinat onle yanveu tenla tense andadid\n",
      "\n",
      "\n",
      "  \n",
      "----\n",
      "iter 88000, loss: 47.783521\n",
      "----\n",
      " hann'g thiti Ta NVK ap hy shilt. mu wa averd,  s  viy, a 2 bigetacarer. GONIRY ­ ISLS NARKIWICRKTES rke PUR. SKY\n",
      "\n",
      "                    whis hore cive lewp!\n",
      "\n",
      "   rilce.\n",
      "        ATMAONn LANENGT 1 rang!\n",
      "\n",
      "  \n",
      "----\n",
      "iter 89000, loss: 45.933334\n",
      "----\n",
      " ?\n",
      "      NARR sHL. roun ald onere bidd.)\n",
      "        thhind.\n",
      "\n",
      "          talng shangat acerties bonont f on Ho The dag thenpm noro ge't arid wha\n",
      "     desar eqpos ales pmetl, ois hopis argot? HH chame ul te  \n",
      "----\n",
      "iter 90000, loss: 45.161150\n",
      "----\n",
      " onbe NE Mok to dloan seiups ay.     oso ind sapoka tinf.\n",
      "\n",
      "\n",
      "       She ntokuyiv ath\n",
      "   Ti tILo\n",
      "          A TEON RWEROR a pank wor onoud camgli hin.\n",
      "                TOER ME'ot t wea'mimt is Cint.\n",
      "  pe o \n",
      "----\n",
      "iter 91000, loss: 43.957798\n",
      "----\n",
      " \n",
      "              i antt. IST.\n",
      "           hhet ome ave beof.\n",
      "\n",
      "      urapr of youllatlky\n",
      "\n",
      "          PERAT PELK\n",
      "        BCANT\n",
      "          LANevet hesedy p LÖCEOR. Iothowd the ak Iale clep lepo fruntilfinsag. \n",
      "----\n",
      "iter 92000, loss: 47.299590\n",
      "----\n",
      "  OEPTHW. MOLI SHor  hits.. GA2 RGARONINE\n",
      "          TOK\n",
      "\n",
      "                     atabod poangottcis.\n",
      "         BARAEND\n",
      "        NCUUNY TOL\n",
      " Juthls.\n",
      "\n",
      "       S chisf RSE thertm?\n",
      "\n",
      "            THIK cbhad reowo  \n",
      "----\n",
      "iter 93000, loss: 45.999744\n",
      "----\n",
      " \n",
      "        gkod.LY\n",
      "\n",
      "       Tous atam. Fnl. ACKI lin jore, toers hs sea  thery yowid ong Fan Sudlirat ma.\n",
      "\n",
      "     S sheit thim the gem.D)\n",
      "         YIICASONI PUNT!.\n",
      "       Thusnop!\n",
      "\n",
      "                 herlegi \n",
      "----\n",
      "iter 94000, loss: 48.342663\n",
      "----\n",
      " ALWERIRQDA ALGS fres f tHOD! HOV. Thadbla helk. o lceversom he ing ing! AO AWILEg\n",
      "            B wari erir.\n",
      "            Jfo thfrocore!\n",
      "            s the telh\n",
      "\n",
      "        CBARFSs cirit ie d?\n",
      "\n",
      "    ERAT BATO \n",
      "----\n",
      "iter 95000, loss: 47.511473\n",
      "----\n",
      " LAongam erary hit peond sliver Fokinl owhired micicetice t A IVER..\n",
      "                  COL NLIRLER. Cendbe, Bamenlt lishime thede cacmit hi  an sun at'lik.\n",
      "     BBOULNG. Dons fe't FTRIGu kint.\n",
      "         \n",
      "----\n",
      "iter 96000, loss: 46.551497\n",
      "----\n",
      " out.\n",
      "                    TARON ke,  t'ge afk ap walr sovoIgt.\n",
      "\n",
      "              (URT ma,           NNIN othe orot PTERRN upinl inrs theats acot celat loush ast, 2           INits. ­ TOND.\n",
      "\n",
      "               \n",
      "----\n",
      "iter 97000, loss: 45.702922\n",
      "----\n",
      " oud an this iil ck\n",
      "            TAT.\n",
      "\n",
      "         pecorns CARY ILIA ANGT Po are fit. LACUULIL ONTIIN thu  ther f anger Ff oki. MLECHA\n",
      "     EXTtPang. LA ndevta gus A TAMACLAN woonbeg bcit IT.\n",
      "         .\n",
      "\n",
      "  \n",
      "----\n",
      "iter 98000, loss: 44.291411\n",
      "----\n",
      " the be tIEY\n",
      "               hind orit buslbissss k iegecee..\n",
      "\n",
      "             Zyalm you\n",
      "\n",
      "                          roedher\n",
      "\n",
      "                   ure#sho fyous tewsrap beg.\n",
      "       (ns  ste BSTOa's wiclyatst  \n",
      "----\n",
      "iter 99000, loss: 43.213988\n",
      "----\n",
      "      L -thot that,.\n",
      "\n",
      "        (HINTT TAD        s ghe aperitist\n",
      "            hest, oucit,.\n",
      "          t  B SHAS thoart TERPT TOY\n",
      "             BAs Mh FER they, NININGT SHo,        LA Bh ashle tout tORES.  \n",
      "----\n",
      "iter 100000, loss: 46.852784\n",
      "----\n",
      "      EUC Whe\n",
      "                     de. INIG ATHA LOICK BERLDHA ANUL HUIER\n",
      "\n",
      "                 hon ret f shive lrete niny tofp.\n",
      "\n",
      "            PINUCRY\n",
      "                sit. THICONIER\n",
      "                  tIY    \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 1000 == 0:\n",
    "    print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
